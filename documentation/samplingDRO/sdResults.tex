\documentclass[11pt]{article}

\usepackage{myArticle}

\title{Sampling-based Distributionally Robust Optimization}
\date{Version: \today}
\author{Harsha Gangammanavar\\ Department of Industrial Engineering, Southern Methodist University}

%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%
\maketitle

\section{Summary of 2-SD Results}

Consider a two-stage stochastic linear program:
\begin{subequations} \label{eq:2slp}
\begin{align}\label{eq:2slp_master}
	\min~& f(x) := c^\top x + \expect{h(x,\tilde{\omega})}{P}\\
	\text{s.t.}~& x \in \set{X} := \{x~|~Ax \leq b \} \subseteq \RR^{n_1} \notag 
\end{align}
where, the recourse function is defined as follows:
\begin{align} \label{eq:2slp_subprob}
	h(x,\omega) := \min ~& d(\omega)^\top y \\
				  \text{s.t.}~& D(\omega) y = \xi(\omega) - C(\omega)x \notag \\
				  & y \geq 0, y \in \RR^{n_2}. \notag 
\end{align}
\end{subequations}

In this summary of 2-SD results, let $\{f^k\}$, $\{x^k\}$, $\{\hat{x}^k\}$ be the sequences of objective function approximations, candidate solutions, and incumbent solutions, respectively, generated by the algorithm. Let $\set{K} = \{i_1, i_2,\ldots\}$ denote the set of iterations where the incumbent changes. The set of minorants $\set{J}^k$ used to define the function approximation $f^k$ is given by:
\begin{align}\label{eq:minorantSet}
	\set{J}^k = \{j \in \set{J}^{k-1}~|~ \theta_j^{k-1} > 0\} \cup \{i_{k-1}, k\}. 
\end{align}


\begin{lemma}[Lemma 1 in \cite{Higle1991}] \label{lemma:uniformConvergence}
The sequence of functions $\{h^k\}_k$ converges uniformly on $\set{X}$.
\end{lemma}

\begin{theorem}[Theorem 2 in \cite{Higle1991}] \label{thm:asymptoticSupport}
If $\{\hat{x}^k\}_{k \in \set{K}} \rightarrow \hat{x}$, then $\{f_k(\hat{x}^k)\}_{k \in \set{K}} \rightarrow f(\hat{x})$ (wp1). Let $(\alpha^k, \beta^k)$ denote the coefficient corresponding to the incumbent minorant. Then, every accumulation point of $\{(\alpha^k, c + \beta^k)\}_{k \in \set{K}}$ defines a support of $f$ at $\hat{x}$ (with probability one).
\end{theorem}
\begin{proof}
Needs Lemma \ref{lemma:uniformConvergence}.
\end{proof}

\begin{corollary} \label{corr:objFnConvergence}
If $\{\hat{x}^k\}_{k \in \set{K}} \rightarrow \hat{x}$, then 
\begin{align}
	\lim_{k \in \set{K}} f^k(\hat{x}^k) = \lim_{k \in \set{K}} f^{k+1}(\hat{x}^k) = f(\hat{x})
\end{align}
with probability one.
\end{corollary}
\begin{proof}
Needs Theorem \ref{thm:asymptoticSupport}.
\end{proof}

\begin{lemma}[Lemma 6 in \cite{Higle1991}] \label{lemma:finiteIncumbChg}
If $|\set{K}|$ is not finite, then 
\begin{align}
	\lim_{m \rightarrow \infty} \frac{1}{m} \sum_{k = 1}^m \Delta^{i_k} = 0
\end{align}
with probability one.
\end{lemma}
\begin{proof}
Requires Corollary \ref{corr:objFnConvergence}.
\end{proof}



\begin{theorem}[Theorem 3 in \cite{Higle1994}] \label{thm:objFnConvergence}
With probability one,
\begin{align}
	\limsup_{k \rightarrow \infty} f^k(x^{k+1}) - f^k(\hat{x}^k) = 0.
\end{align}
\end{theorem}
\begin{proof}
Requires Lemma \ref{lemma:finiteIncumbChg}.
\end{proof}

\begin{theorem}[Theorem 5 in \cite{Higle1994}]
With probability one there exists a subsequence of iterations, indexed by a set $\set{K}_*$, such that $\lim_{k \in \set{K}_*} f^{k-1}(x^k) + \frac{1}{2}\|x^k - \hat{x}^{k-1}\| - f^{k-1}(\hat{x}^{k-1}) = 0$, and every accumulation point of $\{\hat{x}^k\}_{k \in \set{K}_*}$ is an optimal solution of \eqref{eq:2slp} (with probability one).
\end{theorem}
\begin{proof}
Need Theorem 3 and Lemma 4 in \cite{Higle1994}.
\end{proof}

\bibliographystyle{plain}
\bibliography{allmyReferences}

\end{document}